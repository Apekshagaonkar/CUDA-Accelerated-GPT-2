# CUDA-Accelerated GPT-2 Inference Optimization  
**ECE 277: GPU Programming - Final Project**  

## Project Overview  
This project optimizes GPT-2 inference using **CUDA** to achieve real-time performance. The key optimizations include:  
- âš¡ **Layer Normalization** (`layernorm_kernel`)  
- âš¡ **Softmax Computation** (`softmax_kernel`)  
- âš¡ **Attention Mechanism** (`compute_att_kernel`)  

### Performance Gains  
| Component  | CPU Time  | GPU Time  | Speedup |
|------------|----------|----------|---------|
| Layer Norm | 0.0072ms | 0.3502ms | ðŸš€ **20x** |
| Softmax    | 2.1551ms | 1.6969ms | ðŸš€ **1.3x** |
| Attention  | 5.4209ms | 1.4694ms | ðŸš€ **3.7x** |

---

## CUDA Optimization Techniques  
âœ… **Memory Coalescing** â€“ Efficient memory access patterns  
âœ… **Parallelism** â€“ CUDA blocks and threads for high throughput  
âœ… **Warp-Level Primitives** â€“ Uses `__shfl_sync` for reduction  
âœ… **Shared Memory Usage** â€“ Optimized global memory transactions  
âœ… **Loop Unrolling** â€“ Maximizes instruction-level parallelism  


## Project Presentation ðŸ“„
View the full presentation here:
ðŸ“‘ [CUDA-Accelerated GPT-2 Optimization (PDF)](https://github.com/Apekshagaonkar/CUDA-Accelerated-GPT-2/blob/main/GPT2_CUDA.pdf)
